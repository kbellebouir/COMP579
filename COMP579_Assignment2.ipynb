{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, temp):\n",
    "    \"\"\"Compute softmax values for action probabilities.\"\"\"\n",
    "    exp_values = np.exp((x - np.max(x)) / temp)  # Subtract max for numerical stability\n",
    "    return exp_values / np.sum(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        \"\"\"Initialize the SARSA agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.temp = temp    # Temperature for softmax exploration\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))  # Q-table\n",
    "\n",
    "    def select_action(self, s, greedy=False):\n",
    "        \"\"\"Select an action using softmax exploration or greedily.\"\"\"\n",
    "        if greedy:\n",
    "            # Choose the action with the highest Q-value (exploitation)\n",
    "            return np.argmax(self.Q[s])\n",
    "        else:\n",
    "            # Use softmax exploration\n",
    "            action_probs = softmax(self.Q[s], self.temp)\n",
    "            return np.random.choice(len(action_probs), p=action_probs)\n",
    "\n",
    "    def update(self, s, a, r, s_prime, a_prime, done):\n",
    "        \"\"\"Update the Q-table using the SARSA update rule.\"\"\"\n",
    "        if done:\n",
    "            target = r  # No next state if episode is done\n",
    "        else:\n",
    "            target = r + self.gamma * self.Q[s_prime, a_prime]\n",
    "        self.Q[s, a] += self.alpha * (target - self.Q[s, a])\n",
    "\n",
    "\n",
    "class ExpectedSarsa:\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        \"\"\"Initialize the Expected SARSA agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.temp = temp    # Temperature for softmax exploration\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))  # Q-table\n",
    "\n",
    "    def select_action(self, s, greedy=False):\n",
    "        \"\"\"Select an action using softmax exploration or greedily.\"\"\"\n",
    "        if greedy:\n",
    "            # Choose the action with the highest Q-value (exploitation)\n",
    "            return np.argmax(self.Q[s])\n",
    "        else:\n",
    "            # Use softmax exploration\n",
    "            action_probs = softmax(self.Q[s], self.temp)\n",
    "            return np.random.choice(len(action_probs), p=action_probs)\n",
    "\n",
    "    def update(self, s, a, r, s_prime, done):\n",
    "        \"\"\"Update the Q-table using the Expected SARSA update rule.\"\"\"\n",
    "        if done:\n",
    "            target = r  # No next state if episode is done\n",
    "        else:\n",
    "            # Compute the expected value of the next state\n",
    "            action_probs = softmax(self.Q[s_prime], self.temp)\n",
    "            expected_value = np.sum(action_probs * self.Q[s_prime])\n",
    "            target = r + self.gamma * expected_value\n",
    "        self.Q[s, a] += self.alpha * (target - self.Q[s, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(env, agent_class, alpha, gamma, temp, num_segments=500, num_training_episodes=10):\n",
    "    \"\"\"Run a single trial and return training and testing returns.\"\"\"\n",
    "    agent = agent_class(env, alpha, gamma, temp)\n",
    "    training_returns = []  # Stores average training returns per segment\n",
    "    testing_returns = []   # Stores testing returns per segment\n",
    "\n",
    "    for segment in range(num_segments):\n",
    "        # Training phase (10 episodes)\n",
    "        training_rewards = []\n",
    "        for _ in range(num_training_episodes):\n",
    "            s, _ = env.reset()\n",
    "            a = agent.select_action(s, greedy=False)\n",
    "            total_reward = 0\n",
    "\n",
    "            while True:\n",
    "                s_prime, r, done, truncated, _ = env.step(a)\n",
    "                a_prime = agent.select_action(s_prime, greedy=False)\n",
    "                if isinstance(agent, ExpectedSarsa):\n",
    "                    agent.update(s, a, r, s_prime, done)\n",
    "                else:\n",
    "                    agent.update(s, a, r, s_prime, a_prime, done)\n",
    "                total_reward += r\n",
    "                s, a = s_prime, a_prime\n",
    "\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "            training_rewards.append(total_reward)\n",
    "\n",
    "        # Testing phase (1 episode)\n",
    "        s, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            a = agent.select_action(s, greedy=True)\n",
    "            s_prime, r, done, truncated, _ = env.step(a)\n",
    "            total_reward += r\n",
    "            s = s_prime\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        # Store results\n",
    "        training_returns.append(np.mean(training_rewards))  # Average training return\n",
    "        testing_returns.append(total_reward)  # Testing return\n",
    "\n",
    "    return training_returns, testing_returns\n",
    "\n",
    "\n",
    "def run_experiment(env, agent_class, alphas, gammas, temps, num_trials=10):\n",
    "    \"\"\"Run experiments for all hyperparameter combinations.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for gamma in gammas:\n",
    "            for temp in temps:\n",
    "                key = f\"alpha={alpha}, gamma={gamma}, temp={temp}\"\n",
    "                print(f\"Running {agent_class.__name__} with {key}\")\n",
    "\n",
    "                final_training_returns_all_trials = []\n",
    "\n",
    "                for trial in range(num_trials):\n",
    "                    training_returns, _ = run_trial(env, agent_class, alpha, gamma, temp)\n",
    "                    # Store the average of the last 10 training returns for this trial\n",
    "                    final_training_returns_all_trials.append(np.mean(training_returns[-10:]))\n",
    "\n",
    "                # Store results for this hyperparameter combination\n",
    "                results[key] = {\n",
    "                    \"alpha\": alpha,\n",
    "                    \"temp\": temp,\n",
    "                    \"mean_return\": np.mean(final_training_returns_all_trials),\n",
    "                    \"min_return\": np.min(final_training_returns_all_trials),\n",
    "                    \"max_return\": np.max(final_training_returns_all_trials),\n",
    "                }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_training_performance(results, title, temps):\n",
    "    \"\"\"Plot the final training performance for different hyperparameters.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Group results by temperature\n",
    "    for temp in temps:\n",
    "        alpha_values = []\n",
    "        mean_returns = []\n",
    "        min_returns = []\n",
    "        max_returns = []\n",
    "\n",
    "        for key, data in results.items():\n",
    "            if data[\"temp\"] == temp:\n",
    "                alpha_values.append(data[\"alpha\"])\n",
    "                mean_returns.append(data[\"mean_return\"])\n",
    "                min_returns.append(data[\"min_return\"])\n",
    "                max_returns.append(data[\"max_return\"])\n",
    "\n",
    "        # Sort by alpha for plotting\n",
    "        sorted_indices = np.argsort(alpha_values)\n",
    "        alpha_values = np.array(alpha_values)[sorted_indices]\n",
    "        mean_returns = np.array(mean_returns)[sorted_indices]\n",
    "        min_returns = np.array(min_returns)[sorted_indices]\n",
    "        max_returns = np.array(max_returns)[sorted_indices]\n",
    "\n",
    "        # Plot mean and uncertainty (min/max)\n",
    "        plt.plot(alpha_values, mean_returns, label=f\"Temp={temp}\")\n",
    "        plt.fill_between(alpha_values, min_returns, max_returns, alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"Learning Rate (alpha)\")\n",
    "    plt.ylabel(\"Final Training Return\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "State space: Discrete(500)\n",
      "Running Sarsa with alpha=0.1, gamma=0.9, temp=0.1\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"State space:\", env.observation_space)\n",
    "\n",
    "# Hyperparameters to test\n",
    "alphas = [0.1, 0.5, 0.9]  # Learning rates\n",
    "gammas = [0.9]            # Discount factor (fixed for simplicity)\n",
    "temps = [0.1, 1.0, 10.0]  # Temperatures for softmax\n",
    "\n",
    "# Run experiments for SARSA and Expected SARSA\n",
    "sarsa_results = run_experiment(env, Sarsa, alphas, gammas, temps)\n",
    "expected_sarsa_results = run_experiment(env, ExpectedSarsa, alphas, gammas, temps)\n",
    "\n",
    "# Plot final training performance\n",
    "plot_final_training_performance(sarsa_results, \"SARSA Final Training Performance\", temps)\n",
    "plot_final_training_performance(expected_sarsa_results, \"Expected SARSA Final Training Performance\", temps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
