{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, temp):\n",
    "    \"\"\"Compute softmax values for action probabilities.\"\"\"\n",
    "    exp_values = np.exp((x - np.max(x)) / temp)  # Subtract max for numerical stability\n",
    "    return exp_values / np.sum(exp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        \"\"\"Initialize the SARSA agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.temp = temp    # Temperature for softmax exploration\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))  # Q-table\n",
    "\n",
    "    def select_action(self, s, greedy=False):\n",
    "        \"\"\"Select an action using softmax exploration or greedily.\"\"\"\n",
    "        if greedy:\n",
    "            # Choose the action with the highest Q-value (exploitation)\n",
    "            return np.argmax(self.Q[s])\n",
    "        else:\n",
    "            # Use softmax exploration\n",
    "            action_probs = softmax(self.Q[s], self.temp)\n",
    "            return np.random.choice(len(action_probs), p=action_probs)\n",
    "\n",
    "    def update(self, s, a, r, s_prime, a_prime, done):\n",
    "        \"\"\"Update the Q-table using the SARSA update rule.\"\"\"\n",
    "        if done:\n",
    "            target = r  # No next state if episode is done\n",
    "        else:\n",
    "            target = r + self.gamma * self.Q[s_prime, a_prime]\n",
    "        self.Q[s, a] += self.alpha * (target - self.Q[s, a])\n",
    "\n",
    "\n",
    "class ExpectedSarsa:\n",
    "    def __init__(self, env, alpha, gamma, temp):\n",
    "        \"\"\"Initialize the Expected SARSA agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.temp = temp    # Temperature for softmax exploration\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))  # Q-table\n",
    "\n",
    "    def select_action(self, s, greedy=False):\n",
    "        \"\"\"Select an action using softmax exploration or greedily.\"\"\"\n",
    "        if greedy:\n",
    "            # Choose the action with the highest Q-value (exploitation)\n",
    "            return np.argmax(self.Q[s])\n",
    "        else:\n",
    "            # Use softmax exploration\n",
    "            action_probs = softmax(self.Q[s], self.temp)\n",
    "            return np.random.choice(len(action_probs), p=action_probs)\n",
    "\n",
    "    def update(self, s, a, r, s_prime, done):\n",
    "        \"\"\"Update the Q-table using the Expected SARSA update rule.\"\"\"\n",
    "        if done:\n",
    "            target = r  # No next state if episode is done\n",
    "        else:\n",
    "            # Compute the expected value of the next state\n",
    "            action_probs = softmax(self.Q[s_prime], self.temp)\n",
    "            expected_value = np.sum(action_probs * self.Q[s_prime])\n",
    "            target = r + self.gamma * expected_value\n",
    "        self.Q[s, a] += self.alpha * (target - self.Q[s, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trial(env, agent_class, alpha, gamma, temp, num_segments=500, num_training_episodes=10):\n",
    "    \"\"\"Run a single trial and return training and testing returns.\"\"\"\n",
    "    agent = agent_class(env, alpha, gamma, temp)\n",
    "    training_returns = []  # Stores average training returns per segment\n",
    "    testing_returns = []   # Stores testing returns per segment\n",
    "\n",
    "    for segment in range(num_segments):\n",
    "        # Training phase (10 episodes)\n",
    "        training_rewards = []\n",
    "        for _ in range(num_training_episodes):\n",
    "            s, _ = env.reset()\n",
    "            a = agent.select_action(s, greedy=False)\n",
    "            total_reward = 0\n",
    "\n",
    "            while True:\n",
    "                s_prime, r, done, truncated, _ = env.step(a)\n",
    "                a_prime = agent.select_action(s_prime, greedy=False)\n",
    "                if isinstance(agent, ExpectedSarsa):\n",
    "                    agent.update(s, a, r, s_prime, done)\n",
    "                else:\n",
    "                    agent.update(s, a, r, s_prime, a_prime, done)\n",
    "                total_reward += r\n",
    "                s, a = s_prime, a_prime\n",
    "\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "            training_rewards.append(total_reward)\n",
    "\n",
    "        # Testing phase (1 episode)\n",
    "        s, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            a = agent.select_action(s, greedy=True)\n",
    "            s_prime, r, done, truncated, _ = env.step(a)\n",
    "            total_reward += r\n",
    "            s = s_prime\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        # Store results\n",
    "        training_returns.append(np.mean(training_rewards))  # Average training return\n",
    "        testing_returns.append(total_reward)  # Testing return\n",
    "\n",
    "    return training_returns, testing_returns\n",
    "\n",
    "\n",
    "def run_experiment(env, agent_class, alphas, gammas, temps, num_trials=10):\n",
    "    \"\"\"Run experiments for all hyperparameter combinations.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for gamma in gammas:\n",
    "            for temp in temps:\n",
    "                key = f\"alpha={alpha}, gamma={gamma}, temp={temp}\"\n",
    "                print(f\"Running {agent_class.__name__} with {key}\")\n",
    "\n",
    "                training_returns_all_trials = []\n",
    "                testing_returns_all_trials = []\n",
    "\n",
    "                for trial in range(num_trials):\n",
    "                    training_returns, testing_returns = run_trial(env, agent_class, alpha, gamma, temp)\n",
    "                    training_returns_all_trials.append(training_returns)\n",
    "                    testing_returns_all_trials.append(testing_returns)\n",
    "\n",
    "                # Store results for this hyperparameter combination\n",
    "                results[key] = {\n",
    "                    \"training_returns\": training_returns_all_trials,\n",
    "                    \"testing_returns\": testing_returns_all_trials,\n",
    "                }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(6)\n",
      "State space: Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Taxi-v3'\n",
    "env = gym.make(env_name)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"State space:\", env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sarsa with alpha=0.1, gamma=0.9, temp=0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m temps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m10.0\u001b[39m]  \u001b[38;5;66;03m# Temperatures for softmax\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Run experiments for SARSA and Expected SARSA\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m sarsa_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSarsa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgammas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m expected_sarsa_results \u001b[38;5;241m=\u001b[39m run_experiment(env, ExpectedSarsa, alphas, gammas, temps)\n",
      "Cell \u001b[1;32mIn[7], line 63\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(env, agent_class, alphas, gammas, temps, num_trials)\u001b[0m\n\u001b[0;32m     60\u001b[0m testing_returns_all_trials \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trials):\n\u001b[1;32m---> 63\u001b[0m     training_returns, testing_returns \u001b[38;5;241m=\u001b[39m \u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     training_returns_all_trials\u001b[38;5;241m.\u001b[39mappend(training_returns)\n\u001b[0;32m     65\u001b[0m     testing_returns_all_trials\u001b[38;5;241m.\u001b[39mappend(testing_returns)\n",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m, in \u001b[0;36mrun_trial\u001b[1;34m(env, agent_class, alpha, gamma, temp, num_segments, num_training_episodes)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     a \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(s, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 35\u001b[0m     s_prime, r, done, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[0;32m     37\u001b[0m     s \u001b[38;5;241m=\u001b[39m s_prime\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\taxi.py:290\u001b[0m, in \u001b[0;36mTaxiEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, a):\n\u001b[0;32m    289\u001b[0m     transitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms][a]\n\u001b[1;32m--> 290\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[43mcategorical_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransitions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_random\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m     p, s, r, t \u001b[38;5;241m=\u001b[39m transitions[i]\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms \u001b[38;5;241m=\u001b[39m s\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\toy_text\\utils.py:8\u001b[0m, in \u001b[0;36mcategorical_sample\u001b[1;34m(prob_n, np_random)\u001b[0m\n\u001b[0;32m      6\u001b[0m prob_n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(prob_n)\n\u001b[0;32m      7\u001b[0m csprob_n \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(prob_n)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsprob_n\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp_random\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:1342\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1341\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters to test\n",
    "alphas = [0.1, 0.5, 0.9]  # Learning rates\n",
    "gammas = [0.9]            # Discount factor (fixed for simplicity)\n",
    "temps = [0.1, 1.0, 10.0]  # Temperatures for softmax\n",
    "\n",
    "# Run experiments for SARSA and Expected SARSA\n",
    "sarsa_results = run_experiment(env, Sarsa, alphas, gammas, temps)\n",
    "expected_sarsa_results = run_experiment(env, ExpectedSarsa, alphas, gammas, temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sarsa_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_final_training_performance(\u001b[43msarsa_results\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSARSA Final Training Performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m plot_final_training_performance(expected_sarsa_results, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected SARSA Final Training Performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sarsa_results' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_final_training_performance(results, title):\n",
    "    \"\"\"Plot the final training performance for different hyperparameters.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Group results by temperature\n",
    "    for temp in temps:\n",
    "        alpha_values = []\n",
    "        mean_returns = []\n",
    "        min_returns = []\n",
    "        max_returns = []\n",
    "\n",
    "        for key, data in results.items():\n",
    "            if data[\"temp\"] == temp:\n",
    "                alpha_values.append(data[\"alpha\"])\n",
    "                mean_returns.append(data[\"mean_return\"])\n",
    "                min_returns.append(data[\"min_return\"])\n",
    "                max_returns.append(data[\"max_return\"])\n",
    "\n",
    "        # Sort by alpha for plotting\n",
    "        sorted_indices = np.argsort(alpha_values)\n",
    "        alpha_values = np.array(alpha_values)[sorted_indices]\n",
    "        mean_returns = np.array(mean_returns)[sorted_indices]\n",
    "        min_returns = np.array(min_returns)[sorted_indices]\n",
    "        max_returns = np.array(max_returns)[sorted_indices]\n",
    "\n",
    "        # Plot mean and uncertainty (min/max)\n",
    "        plt.plot(alpha_values, mean_returns, label=f\"Temp={temp}\")\n",
    "        plt.fill_between(alpha_values, min_returns, max_returns, alpha=0.2)\n",
    "\n",
    "    plt.xlabel(\"Learning Rate (alpha)\")\n",
    "    plt.ylabel(\"Final Training Return\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_final_training_performance(sarsa_results, \"SARSA Final Training Performance\")\n",
    "plot_final_training_performance(expected_sarsa_results, \"Expected SARSA Final Training Performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
